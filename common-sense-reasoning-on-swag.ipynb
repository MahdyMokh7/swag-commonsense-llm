{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWAG MCQ Reasoning with BERT and In-Context Learning\n",
    "\n",
    "This notebook implements **Task 3** from the *Introduction to Data Science - Assignment 5&6*. The goal is to explore how **Large Language Models (LLMs)**, specifically BERT, can solve **multiple-choice commonsense reasoning questions** from the **SWAG dataset**.\n",
    "\n",
    "Traditional machine learning models struggle with nuanced semantic inference tasks. Here, we utilize modern LLM techniques‚Äîincluding **zero-shot prediction**, **in-context learning (ICL)**, and **fine-tuning with LoRA**‚Äîto handle the task effectively.\n",
    "\n",
    "### üîç Task Overview\n",
    "We work with the **SWAG dataset** (113k+ MCQs) to:\n",
    "- Preprocess and tokenize inputs in multiple-choice format.\n",
    "- Evaluate BERT in a zero-shot setting.\n",
    "- Apply in-context learning using prompt engineering.\n",
    "- Fine-tune the BERT model with LoRA for performance gains.\n",
    "- Compare model performance across zero-shot, ICL, and fine-tuned configurations.\n",
    "\n",
    "### üß† Learning Objective\n",
    "- Understand the limitations of classical models for reasoning tasks.\n",
    "- Apply prompt-based and fine-tuning strategies for LLMs.\n",
    "- Use HuggingFace Transformers to load, preprocess, evaluate, and fine-tune models.\n",
    "- Analyze results using accuracy, confusion matrix, and perplexity.\n",
    "\n",
    "**Dataset:** [SWAG - Situations With Adversarial Generations](https://huggingface.co/datasets/allenai/swag)\n",
    "            \n",
    "### üìå Note\n",
    "Make sure to\n",
    "- Use your Hugging Face token for access.\n",
    "- Run on Kaggle for reliable GPU access and smoother performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1) Uninstall any leftovers\n",
    "!pip uninstall -y transformers tokenizers\n",
    "\n",
    "# 2) Install the last version before the helpers were removed\n",
    "!pip install --no-cache-dir transformers==4.47.0\n",
    "\n",
    "# 3) Now install your other libraries (they won‚Äôt bump Transformers because 4.47.0 satisfies PEFT)\n",
    "!pip install --no-cache-dir peft evaluate packaging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hugging Face Access Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hugging Face Login\n",
    "from huggingface_hub import notebook_login, login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the SWAG dataset\n",
    "dataset = load_dataset(\"swag\", \"regular\")\n",
    "\n",
    "df_train = dataset['train'].to_pandas()\n",
    "df_val = dataset['validation'].to_pandas()\n",
    "df_test = dataset['test'].to_pandas()\n",
    "\n",
    "# adding a column to specify the split\n",
    "df_train['split'] = 'train'\n",
    "df_val['split'] = 'validation'\n",
    "df_test['split'] = 'test'\n",
    "# Concatenate all into one DataFrame\n",
    "df = pd.concat([df_train, df_val, df_test], ignore_index=True)\n",
    "\n",
    "print(dataset)\n",
    "dataset[\"train\"][0]\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyza the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(x='label', data=df)\n",
    "plt.title(\"Distribution of Correct Answer Labels\")\n",
    "plt.xlabel(\"Correct Ending (0-3)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg length of sent1, sent2, ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['sent1_len'] = df['sent1'].apply(lambda x: len(x.split()))\n",
    "df['sent2_len'] = df['sent2'].apply(lambda x: len(x.split()))\n",
    "for i in range(4):\n",
    "    df[f'ending{i}_len'] = df[f'ending{i}'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "# Display averages\n",
    "print(\"Average lengths:\")\n",
    "print(f\"sent1: {df['sent1_len'].mean():.2f} words\")\n",
    "print(f\"sent2: {df['sent2_len'].mean():.2f} words\")\n",
    "for i in range(4):\n",
    "    print(f\"ending{i}: {df[f'ending{i}_len'].mean():.2f} words\")\n",
    "    \n",
    "# Display max\n",
    "print(\"Max lengths:\")\n",
    "print(f\"sent1: {df['sent1_len'].max():.2f} words\")\n",
    "print(f\"sent2: {df['sent2_len'].max():.2f} words\")\n",
    "for i in range(4):    \n",
    "    print(f\"ending{i}: {df[f'ending{i}_len'].max():.2f} words\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One full mcq example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(\"sent1:\", df.loc[i, 'sent1'])\n",
    "print(\"sent2:\", df.loc[i, 'sent2'])\n",
    "for j in range(4):\n",
    "    print(f\"ending{j}:\", df.loc[i, f'ending{j}'])\n",
    "    print(\"Correct label:\", df.loc[i, 'label'])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# loading tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_swag(batch):\n",
    "    first_sentences = [s1 for s1 in batch['sent1'] for _ in range(4)]\n",
    "    second_sentences = [\n",
    "        batch['sent2'][i] + \" \" + batch[f'ending{j}'][i]\n",
    "        for i in range(len(batch['sent1']))\n",
    "        for j in range(4)\n",
    "    ]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        first_sentences,\n",
    "        second_sentences,\n",
    "        truncation=True,\n",
    "        padding=\"longest\"\n",
    "        )\n",
    "    # Unflatten: group every 4 items as a list\n",
    "    def regroup(values):\n",
    "        return [values[i:i + 4] for i in range(0, len(values), 4)]\n",
    "    \n",
    "    return {key: regroup(val) for key, val in tokenized.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Apply the preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Apply preprocessing function with batch\n",
    "tokenized_dataset = {\n",
    "    split: dataset[split].map(preprocess_swag, batched=True)\n",
    "    for split in [\"train\", \"validation\", \"test\"]\n",
    "}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# clean and finalize the dataset\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    if \"label\" in tokenized_dataset[split].column_names:\n",
    "        tokenized_dataset[split] = tokenized_dataset[split].rename_column(\"label\", \"labels\")\n",
    "        \n",
    "        keep_cols = ['input_ids', 'attention_mask', 'token_type_ids', 'labels']\n",
    "        tokenized_dataset[split] = tokenized_dataset[split].remove_columns(\n",
    "            [col for col in tokenized_dataset[split].column_names if col not in keep_cols]\n",
    "        )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForMultipleChoice\n",
    "\n",
    "data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMultipleChoice\n",
    "\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\"google-bert/bert-base-uncased\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Model on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# This will store (method, accuracy)\n",
    "icl_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test case analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Device setup (should match your model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Pick a single example to inspect\n",
    "i = 0  # You can change this index to test other samples\n",
    "example = tokenized_dataset[\"validation\"][i]\n",
    "raw_example = dataset[\"validation\"][i]  # un-tokenized version for readable display\n",
    "\n",
    "# Convert inputs to tensor format (batch size = 1) and move to deviceinput = {\n",
    "k: torch.tensor([example[k]]).to(device)\n",
    "for k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n",
    "}\n",
    "\n",
    "# Run the model and get prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**input)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "# Display the context\n",
    "context = raw_example[\"sent1\"] + \" \" + raw_example[\"sent2\"]print(f\"üìå Context: {context}\")\n",
    "# Display all choices with prediction and ground truth\n",
    "print(\"üîò Options:\")\n",
    "for j in range(4):\n",
    "option_text = raw_example[f\"ending{j}\"]\n",
    "prefix = \"‚úÖ\" if j == raw_example[\"label\"] else \"‚ùå\"\n",
    "marker = \"üëâ\" if j == predicted_class else \"  \"\n",
    "print(f\"{marker} {prefix} Choice {j}: {option_text}\")\n",
    "\n",
    "# Final Summary\n",
    "print(f\"üß† Model Prediction: Choice {predicted_class}\")\n",
    "print(f\"‚úîÔ∏è Ground Truth: Choice {raw_example['label']}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full valildation set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_loader = DataLoader(\n",
    "    tokenized_dataset[\"validation\"],\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator)\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for batch in tqdm(val_loader, desc=\"Evaluating Baseline\"):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "        total += batch[\"labels\"].size(0)\n",
    "        \n",
    "baseline_acc = 100*(correct / total)\n",
    "print(f\"üìä Baseline Accuracy: {baseline_acc:.2f}%\")\n",
    "icl_results.append((\"Baseline (No ICL)\", baseline_acc))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. In-Context Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMultipleChoice\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForMultipleChoice\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Select 5-shot ICL examples from validation set\n",
    "fewshot_indices = [0, 1, 2, 3, 4]\n",
    "fewshot_examples = [dataset[\"validation\"][i] for i in fewshot_indices]\n",
    "\n",
    "# Build the static few-shot prompt\n",
    "fewshot_prompt = \"\"\n",
    "for ex in fewshot_examples:\n",
    "    fewshot_prompt += f\"Context: {ex['sent1']} {ex['sent2']}\\n\"\n",
    "    for j in range(4):\n",
    "        fewshot_prompt += f\"{chr(65+j)}. {ex[f'ending{j}']}\\n\"\n",
    "    fewshot_prompt += f\"Answer: {chr(65 + ex['label'])}\\n\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the ICL preprocessing function\n",
    "def preprocess_fewshot_icl_batch(batch):\n",
    "    prompts = []\n",
    "    first_sentences = []\n",
    "    second_sentences = []\n",
    "\n",
    "    for i in range(len(batch['sent1'])):\n",
    "        prompt = fewshot_prompt + f\"\\nContext: {batch['sent1'][i]} {batch['sent2'][i]}\"\n",
    "        first_sentences.extend([prompt] * 4)\n",
    "        second_sentences.extend([\n",
    "            f\"{chr(65 + j)}. {batch[f'ending{j}'][i]}\" for j in range(4)\n",
    "        ])\n",
    "\n",
    "    # print(first_sentences)\n",
    "    # print(second_sentences)\n",
    "    tokenized = tokenizer(first_sentences, second_sentences, truncation=True, padding=\"longest\")\n",
    "\n",
    "    def regroup(values): return [values[i:i+4] for i in range(0, len(values), 4)]\n",
    "    result = {k: regroup(v) for k, v in tokenized.items()}\n",
    "    result[\"labels\"] = batch[\"label\"]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fewshot_icl_dataset = dataset[\"validation\"].map(preprocess_fewshot_icl_batch, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1) Clean & finalize the few-shot ICL dataset\n",
    "\n",
    "# If the old 'label' column still exists, drop it\\nif \"label\" in fewshot_icl_dataset.column_names:\n",
    "fewshot_icl_dataset = fewshot_icl_dataset.remove_columns([\"label\"])\n",
    "\n",
    "# Now keep only the required MCQ fields plus 'labels'\n",
    "keep_cols = [\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"]\n",
    "fewshot_icl_dataset = fewshot_icl_dataset.remove_columns(\n",
    "    [c for c in fewshot_icl_dataset.column_names if c not in keep_cols]\n",
    ")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2) Create DataLoader with your MCQ collator\n",
    "fewshot_icl_loader = DataLoader(\n",
    "    fewshot_icl_dataset,\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# 3) Evaluate Few-Shot ICL\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(fewshot_icl_loader, desc=\"Evaluating Few-Shot ICL\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (preds == batch[\"labels\"]).sum().item()\n",
    "        total += batch[\"labels\"].size(0)\n",
    "    \n",
    "few_shot_accuracy = 100*(correct / total)\n",
    "print(f\"üìä Few-Shot ICL Accuracy: {few_shot_accuracy:.2f}%\")\n",
    "icl_results.append((\"Few shot learning (ICL)\", few_shot_accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot with chain of thought (COT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1) Build the CoT prompt prefix\n",
    "cot_prefix = \"Let's think step by step.\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2) Define the CoT preprocessing fn\n",
    "def preprocess_cot_batch(batch):\n",
    "    first_sentences = []\n",
    "    second_sentences = []\n",
    "\n",
    "    for i in range(len(batch[\"sent1\"])):\n",
    "        # prepend the chain-of-thought instruction before each example\n",
    "        prompt = cot_prefix + f\"Context: {batch['sent1'][i]} {batch['sent2'][i]}\"\n",
    "        # repeat for the 4 choices\n",
    "        first_sentences.extend([prompt] * 4)\n",
    "        second_sentences.extend([\n",
    "            f\"{chr(65 + j)}. {batch[f'ending{j}'][i]}\" for j in range(4)\n",
    "        ])\n",
    "\n",
    "    # tokenize exactly as before\n",
    "    tokenized = tokenizer(first_sentences, second_sentences,\n",
    "                          truncation=True, padding=\"longest\")\n",
    "\n",
    "    # regroup back into (batch_size, 4, seq_len)\n",
    "    def regroup(vals): return [vals[i : i + 4] for i in range(0, len(vals), 4)]\n",
    "    result = {k: regroup(v) for k, v in tokenized.items()}\n",
    "    # carry label forward\n",
    "    result[\"labels\"] = batch[\"label\"]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 3) Map over validation set\n",
    "cot_dataset = dataset[\"validation\"].map(\n",
    "    preprocess_cot_batch,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"validation\"].column_names  # drop raw cols\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 4) Clean up columns (drop old 'label' if still there)\n",
    "if \"label\" in cot_dataset.column_names:\n",
    "    cot_dataset = cot_dataset.remove_columns([\"label\"])\n",
    "keep = [\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"]\n",
    "cot_dataset = cot_dataset.remove_columns(\n",
    "    [c for c in cot_dataset.column_names if c not in keep]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 5) Create a DataLoader\n",
    "cot_loader = DataLoader(\n",
    "    cot_dataset,\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# 6) Evaluate Zero-Shot CoT\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(cot_loader, desc=\"Evaluating Zero-Shot CoT\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (preds == batch[\"labels\"]).sum().item()\n",
    "        total += batch[\"labels\"].size(0)\n",
    "\n",
    "cot_accuracy = 100*(correct / total)\n",
    "print(f\"\\nüìä Zero-Shot CoT Accuracy: {cot_accuracy:.2f}%\")\n",
    "icl_results.append((\"Chain of Thought (ICL)\", cot_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "comparison_df = pd.DataFrame(icl_results, columns=[\"Method\", \"Accuracy\"])\n",
    "comparison_df = comparison_df.sort_values(\"Accuracy\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(data=comparison_df, x=\"Accuracy\", y=\"Method\", palette=\"crest\")\n",
    "plt.title(\"Comparison of ICL Methods on SWAG Validation Set\")\n",
    "plt.xlim(0, 100)\n",
    "plt.xlabel(\"Validation Accuracy\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Fine-Tune Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer, SchedulerType\n",
    "from peft       import LoraConfig, get_peft_model, TaskType\n",
    "import numpy    as np\n",
    "from evaluate   import load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_ds = tokenized_dataset[\"train\"]\n",
    "eval_ds  = tokenized_dataset[\"validation\"]\n",
    "\n",
    "base_model = model\n",
    "base_model.to(device)\n",
    "\n",
    "# Config (wrap bert with lora)\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    ")\n",
    "model_with_lora = get_peft_model(base_model, lora_cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fine-tune visible progress (Transformers 4.47 compatible)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "from transformers import (\n",
    "    TrainingArguments, Trainer,\n",
    "    IntervalStrategy, SchedulerType,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from transformers.utils import logging as hf_logging\n",
    "\n",
    "# Make logs visible in the cell; silence wandb noise\n",
    "hf_logging.set_verbosity_info()\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"   # or: os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "\n",
    "# Metrics\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds  = np.argmax(pred.predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "# Training arguments (step-level eval/save + frequent logging)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"swag_lora_ft\",\n",
    "\n",
    "    do_eval=True,\n",
    "    eval_strategy=IntervalStrategy.STEPS,   # 4.47 naming\n",
    "    eval_steps=500,\n",
    "    save_strategy=IntervalStrategy.STEPS,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    learning_rate=4e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=SchedulerType.LINEAR,\n",
    "\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    # visible logs / progress\n",
    "    logging_strategy=IntervalStrategy.STEPS,\n",
    "    logging_steps=50,         # print every 50 steps\n",
    "    disable_tqdm=False,       # show progress bar\n",
    "    report_to=\"none\",         # no wandb/tensorboard\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "# Tiny callback to guarantee prints even if buffering happens\n",
    "class PrintCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            print(f\"[step {state.global_step}] {logs}\")\n",
    "\n",
    "# Trainer (reuses your existing objects: model_with_lora, train_ds, eval_ds, data_collator, tokenizer)\n",
    "trainer = Trainer(\n",
    "    model=model_with_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,          # ok; may show a future deprecation warning\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[PrintCallback()],\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate & report\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "# Perplexity & confusion matrix\n",
    "perplexity = np.exp(metrics[\"eval_loss\"])\n",
    "print(f\"‚Üí Validation Accuracy:  {100*(metrics['eval_accuracy']):.2f}%\")\n",
    "print(f\"‚Üí Validation Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "preds_output = trainer.predict(eval_ds)\n",
    "preds = np.argmax(preds_output.predictions, axis=1)\n",
    "cm = confusion_matrix(preds_output.label_ids, preds)\n",
    "print(\"‚Üí Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ft_accuracy = 100*(metrics[\"eval_accuracy\"])\n",
    "icl_results.append((\"Fine-Tuned BERT + LoRA\", ft_accuracy))\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(icl_results, columns=[\"Method\", \"Accuracy\"])\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Context Learning with the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import math\n",
    "\n",
    "# make sure model is on device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_with_lora.to(device)\n",
    "\n",
    "\n",
    "def evaluate_loader(model, loader, description):\n",
    "    model.eval()            # set eval mode\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss, total_examples = 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=description):\n",
    "            batch = {k: v.to(device) for k,v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss.item()\n",
    "            bs = batch[\"labels\"].size(0)\n",
    "            total_loss    += loss * bs\n",
    "            total_examples+= bs\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(batch[\"labels\"].cpu().tolist())\n",
    "\n",
    "    avg_loss   = total_loss / total_examples\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    acc        = sum(p==l for p,l in zip(all_preds, all_labels)) / len(all_labels)\n",
    "\n",
    "    print(f\"\\nüìä {description} Accuracy:    {100*(acc):.4f}%\")\n",
    "    print(f\"üìä {description} Perplexity: {perplexity:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(f\"\\n{description} Confusion Matrix:\\n{cm}\\n\")\n",
    "\n",
    "    cr = classification_report(all_labels, all_preds, digits=4)\n",
    "    print(f\"{description} Classification Report:\\n{cr}\\n\")\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot ICL with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fs_acc = evaluate_loader(model_with_lora, fewshot_icl_loader, \"Fine-Tuned Few-Shot ICL\")\n",
    "icl_results.append((\"Fine-Tuned Few-Shot ICL\", fs_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain of thought ICL with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cot_acc = evaluate_loader(model_with_lora, cot_loader, \"Fine-Tuned CoT ICL\")\n",
    "icl_results.append((\"Fine-Tuned CoT ICL\", cot_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results_df = pd.DataFrame(icl_results, columns=[\"Method\", \"Accuracy\"])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Analyze the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare the test datasets\n",
    "\n",
    "# --- a) Baseline test (no ICL, pretrained BERT) ---\n",
    "test_baseline = tokenized_dataset[\"test\"]\n",
    "test_baseline_loader = DataLoader(test_baseline, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "# --- b) Few-Shot ICL on test ---\n",
    "test_fewshot = dataset[\"test\"].map(\n",
    "    preprocess_fewshot_icl_batch,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"test\"].column_names\n",
    ")\n",
    "# drop old label column if it exists, keep only MCQ fields\n",
    "if \"label\" in test_fewshot.column_names:\n",
    "    test_fewshot = test_fewshot.remove_columns([\"label\"])\n",
    "keep = [\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"]\n",
    "test_fewshot = test_fewshot.remove_columns([c for c in test_fewshot.column_names if c not in keep])\n",
    "test_fewshot_loader = DataLoader(test_fewshot, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "# --- c) Chain-of-Thought ICL on test ---\n",
    "test_cot = dataset[\"test\"].map(\n",
    "    preprocess_cot_batch,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"test\"].column_names\n",
    ")\n",
    "if \"label\" in test_cot.column_names:\n",
    "    test_cot = test_cot.remove_columns([\"label\"])\n",
    "test_cot = test_cot.remove_columns([c for c in test_cot.column_names if c not in keep])\n",
    "test_cot_loader = DataLoader(test_cot, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "\n",
    "test_results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# a) Baseline\n",
    "baseline_test_acc = evaluate_loader(base_model, test_baseline_loader, \"Test Baseline\")\n",
    "test_results.append((\"Baseline (No ICL)\", baseline_test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# b) Few‚ÄêShot ICL (pre-fine-tune)\n",
    "fs_test_acc = evaluate_loader(base_model, test_fewshot_loader, \"Test Few-Shot ICL\")\n",
    "test_results.append((\"Few-Shot ICL (pre-FT)\", fs_test_acc))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# c) CoT ICL (pre-fine-tune)\n",
    "cot_test_acc = evaluate_loader(base_model, test_cot_loader, \"Test CoT ICL\")\n",
    "test_results.append((\"CoT ICL (pre-FT)\", cot_test_acc))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# d) Fine-tuned BERT + LoRA (no ICL)\n",
    "ft_test_acc = evaluate_loader(model_with_lora, test_baseline_loader, \"Test Fine-Tuned BERT+LoRA\")\n",
    "test_results.append((\"Fine-Tuned BERT+LoRA\", ft_test_acc))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# e) Few-Shot ICL on fine-tuned model\n",
    "fs_ft_test_acc = evaluate_loader(model_with_lora, test_fewshot_loader, \"Test Fine-Tuned Few-Shot ICL\")\n",
    "test_results.append((\"Few-Shot ICL (FT)\", fs_ft_test_acc))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# f) CoT ICL on fine-tuned model\n",
    "cot_ft_test_acc = evaluate_loader(model_with_lora, test_cot_loader, \"Test Fine-Tuned CoT ICL\")\n",
    "test_results.append((\"CoT ICL (FT)\", cot_ft_test_acc))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(test_results, columns=[\"Method\", \"Test Accuracy\"])\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
